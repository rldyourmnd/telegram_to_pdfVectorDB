import json
import os
import re
from datetime import datetime
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.units import inch
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.fonts import addMapping

def sanitize_filename(name):
    """Clean filename from invalid characters"""
    if name is None:
        return "Unknown"
    return re.sub(r'[<>:"/\\|?*]', '_', str(name))

def convert_emojis_to_text(text):
    """Convert common emojis to text descriptions for better text processing"""
    emoji_map = {
        'üòÄ': '[smile]', 'üòÉ': '[joy]', 'üòÑ': '[laugh]', 'üòÅ': '[grin]', 'üòÜ': '[laughing]',
        'üòÖ': '[sweat_smile]', 'ü§£': '[rofl]', 'üòÇ': '[joy_tears]', 'üôÇ': '[slight_smile]', 'üôÉ': '[upside_down]',
        'üòâ': '[wink]', 'üòä': '[blush]', 'üòá': '[innocent]', 'ü•∞': '[heart_eyes]', 'üòç': '[heart_eyes]',
        'ü§©': '[star_struck]', 'üòò': '[kiss]', 'üòó': '[kissing]', '‚ò∫Ô∏è': '[relaxed]', 'üòö': '[kissing_closed_eyes]',
        'üòô': '[kissing_smiling_eyes]', 'ü•≤': '[smiling_tear]', 'üòã': '[yum]', 'üòõ': '[stuck_out_tongue]', 'üòú': '[stuck_out_tongue_winking_eye]',
        'ü§™': '[zany_face]', 'üòù': '[stuck_out_tongue_closed_eyes]', 'ü§ë': '[money_mouth]', 'ü§ó': '[hugs]', 'ü§≠': '[hand_over_mouth]',
        'ü§´': '[shushing]', 'ü§î': '[thinking]', 'ü§ê': '[zipper_mouth]', 'ü§®': '[raised_eyebrow]', 'üòê': '[neutral]',
        'üòë': '[expressionless]', 'üò∂': '[no_mouth]', 'üòè': '[smirk]', 'üòí': '[unamused]', 'üôÑ': '[eye_roll]',
        'üò¨': '[grimacing]', 'ü§•': '[lying]', 'üòî': '[pensive]', 'üòï': '[confused]', 'üôÅ': '[slight_frown]',
        '‚òπÔ∏è': '[frowning]', 'üò£': '[persevere]', 'üòñ': '[confounded]', 'üò´': '[tired]', 'üò©': '[weary]',
        'ü•∫': '[pleading]', 'üò¢': '[cry]', 'üò≠': '[sob]', 'üò§': '[huff]', 'üò†': '[angry]',
        'üò°': '[rage]', 'ü§¨': '[swearing]', 'ü§Ø': '[exploding_head]', 'üò≥': '[flushed]', 'ü•µ': '[hot]',
        'ü•∂': '[cold]', 'üò±': '[scream]', 'üò®': '[fearful]', 'üò∞': '[anxious]', 'üò•': '[disappointed_relieved]',
        'üòì': '[cold_sweat]', 'ü§ó': '[hugs]', 'ü§î': '[thinking]', 'ü§´': '[shushing]', 'ü§≠': '[hand_over_mouth]',
        'üôà': '[see_no_evil]', 'üôâ': '[hear_no_evil]', 'üôä': '[speak_no_evil]', 'üíÄ': '[skull]', '‚ò†Ô∏è': '[skull_crossbones]',
        'üëª': '[ghost]', 'üëΩ': '[alien]', 'ü§ñ': '[robot]', 'üí©': '[poop]', 'üò∫': '[smiley_cat]',
        'üò∏': '[smile_cat]', 'üòπ': '[joy_cat]', 'üòª': '[heart_eyes_cat]', 'üòº': '[smirk_cat]', 'üòΩ': '[kissing_cat]',
        'üôÄ': '[scream_cat]', 'üòø': '[crying_cat]', 'üòæ': '[pouting_cat]', '‚ù§Ô∏è': '[red_heart]', 'üß°': '[orange_heart]',
        'üíõ': '[yellow_heart]', 'üíö': '[green_heart]', 'üíô': '[blue_heart]', 'üíú': '[purple_heart]', 'üñ§': '[black_heart]',
        'ü§ç': '[white_heart]', 'ü§é': '[brown_heart]', 'üíî': '[broken_heart]', '‚ù£Ô∏è': '[heart_exclamation]', 'üíï': '[two_hearts]',
        'üíû': '[revolving_hearts]', 'üíì': '[heartbeat]', 'üíó': '[growing_heart]', 'üíñ': '[sparkling_heart]', 'üíò': '[cupid]',
        'üíù': '[gift_heart]', 'üíü': '[heart_decoration]', '‚òÆÔ∏è': '[peace]', '‚úùÔ∏è': '[cross]', '‚ò™Ô∏è': '[crescent]',
        'üïâÔ∏è': '[om]', '‚ò∏Ô∏è': '[dharma]', '‚ú°Ô∏è': '[star_of_david]', 'üîØ': '[six_pointed_star]', 'üïé': '[menorah]',
        '‚òØÔ∏è': '[yin_yang]', '‚ò¶Ô∏è': '[orthodox_cross]', 'üõê': '[place_of_worship]', '‚õé': '[ophiuchus]', '‚ôà': '[aries]',
        '‚ôâ': '[taurus]', '‚ôä': '[gemini]', '‚ôã': '[cancer]', '‚ôå': '[leo]', '‚ôç': '[virgo]',
        '‚ôé': '[libra]', '‚ôè': '[scorpio]', '‚ôê': '[sagittarius]', '‚ôë': '[capricorn]', '‚ôí': '[aquarius]',
        '‚ôì': '[pisces]', 'üÜî': '[id]', '‚öõÔ∏è': '[atom]', 'üâë': '[accept]', '‚ò¢Ô∏è': '[radioactive]',
        '‚ò£Ô∏è': '[biohazard]', 'üì¥': '[mobile_phone_off]', 'üì≥': '[vibration_mode]', 'üà∂': '[not_free_of_charge]', 'üàö': '[free_of_charge]',
        'üà∏': '[application]', 'üà∫': '[open_for_business]', 'üà∑Ô∏è': '[monthly_amount]', '‚ú¥Ô∏è': '[eight_pointed_star]', 'üÜö': '[vs]',
        'üíÆ': '[white_flower]', 'üâê': '[bargain]', '„äôÔ∏è': '[secret]', '„äóÔ∏è': '[congratulations]', 'üà¥': '[passing_grade]',
        'üàµ': '[no_vacancy]', 'üàπ': '[discount]', 'üà≤': '[prohibited]', 'üÖ∞Ô∏è': '[a_button]', 'üÖ±Ô∏è': '[b_button]',
        'üÜé': '[ab_button]', 'üÜë': '[cl_button]', 'üÖæÔ∏è': '[o_button]', 'üÜò': '[sos]', '‚ùå': '[cross_mark]',
        '‚≠ï': '[heavy_large_circle]', 'üõë': '[stop_sign]', '‚õî': '[no_entry]', 'üìõ': '[name_badge]', 'üö´': '[prohibited]',
        'üíØ': '[hundred]', 'üí¢': '[anger]', '‚ô®Ô∏è': '[hot_springs]', 'üö∑': '[no_pedestrians]', 'üöØ': '[no_littering]',
        'üö≥': '[no_bicycles]', 'üö±': '[non_potable_water]', 'üîû': '[no_one_under_eighteen]', 'üìµ': '[no_mobile_phones]', 'üö≠': '[no_smoking]',
        '‚ùó': '[exclamation]', '‚ùï': '[white_exclamation]', '‚ùì': '[question]', '‚ùî': '[white_question]', '‚ÄºÔ∏è': '[double_exclamation]',
        '‚ÅâÔ∏è': '[interrobang]', 'üîÖ': '[low_brightness]', 'üîÜ': '[high_brightness]', '„ÄΩÔ∏è': '[part_alternation_mark]', '‚ö†Ô∏è': '[warning]',
        'üö∏': '[children_crossing]', 'üî±': '[trident]', '‚öúÔ∏è': '[fleur_de_lis]', 'üî∞': '[japanese_symbol_for_beginner]', '‚ôªÔ∏è': '[recycling]',
        '‚úÖ': '[check_mark]', 'üàØ': '[reserved]', 'üíπ': '[chart_increasing_with_yen]', '‚ùáÔ∏è': '[sparkle]', '‚ú≥Ô∏è': '[eight_spoked_asterisk]',
        '‚ùé': '[cross_mark_button]', 'üåê': '[globe_with_meridians]', 'üí†': '[diamond_with_a_dot]', '‚ìÇÔ∏è': '[circled_m]', 'üåÄ': '[cyclone]',
        'üí§': '[zzz]', 'üèß': '[atm]', 'üöæ': '[water_closet]', '‚ôø': '[wheelchair]', 'üÖøÔ∏è': '[p_button]',
        'üà≥': '[vacancy]', 'üàÇÔ∏è': '[service_charge]', 'üõÇ': '[passport_control]', 'üõÉ': '[customs]', 'üõÑ': '[baggage_claim]',
        'üõÖ': '[left_luggage]', 'üöπ': '[mens]', 'üö∫': '[womens]', 'üöº': '[baby_symbol]', 'üöª': '[restroom]',
        'üöÆ': '[litter_in_bin]', 'üé¶': '[cinema]', 'üì∂': '[signal_strength]', 'üàÅ': '[here]', 'üî£': '[symbols]',
        '‚ÑπÔ∏è': '[information]', 'üî§': '[abc]', 'üî°': '[abcd]', 'üî†': '[capital_abcd]', 'üÜñ': '[ng_button]',
        'üÜó': '[ok_button]', 'üÜô': '[up_button]', 'üÜí': '[cool_button]', 'üÜï': '[new_button]', 'üÜì': '[free_button]',
        '0Ô∏è‚É£': '[keycap_0]', '1Ô∏è‚É£': '[keycap_1]', '2Ô∏è‚É£': '[keycap_2]', '3Ô∏è‚É£': '[keycap_3]', '4Ô∏è‚É£': '[keycap_4]',
        '5Ô∏è‚É£': '[keycap_5]', '6Ô∏è‚É£': '[keycap_6]', '7Ô∏è‚É£': '[keycap_7]', '8Ô∏è‚É£': '[keycap_8]', '9Ô∏è‚É£': '[keycap_9]',
        'üîü': '[keycap_10]', 'üî¢': '[input_numbers]', '#Ô∏è‚É£': '[hash]', '*Ô∏è‚É£': '[asterisk]', '‚èèÔ∏è': '[eject]',
        '‚ñ∂Ô∏è': '[play]', '‚è∏Ô∏è': '[pause]', '‚èØÔ∏è': '[play_pause]', '‚èπÔ∏è': '[stop]', '‚è∫Ô∏è': '[record]',
        '‚è≠Ô∏è': '[next_track]', '‚èÆÔ∏è': '[previous_track]', '‚è©': '[fast_forward]', '‚è™': '[rewind]', '‚è´': '[fast_up]',
        '‚è¨': '[fast_down]', '‚óÄÔ∏è': '[reverse]', 'üîº': '[up_button]', 'üîΩ': '[down_button]', '‚û°Ô∏è': '[right_arrow]',
        '‚¨ÖÔ∏è': '[left_arrow]', '‚¨ÜÔ∏è': '[up_arrow]', '‚¨áÔ∏è': '[down_arrow]', '‚ÜóÔ∏è': '[up_right_arrow]', '‚ÜòÔ∏è': '[down_right_arrow]',
        '‚ÜôÔ∏è': '[down_left_arrow]', '‚ÜñÔ∏è': '[up_left_arrow]', '‚ÜïÔ∏è': '[up_down_arrow]', '‚ÜîÔ∏è': '[left_right_arrow]', '‚Ü™Ô∏è': '[left_arrow_curving_right]',
        '‚Ü©Ô∏è': '[right_arrow_curving_left]', '‚§¥Ô∏è': '[right_arrow_curving_up]', '‚§µÔ∏è': '[right_arrow_curving_down]', 'üîÄ': '[twisted_rightwards_arrows]', 'üîÅ': '[repeat]',
        'üîÇ': '[repeat_single]', 'üîÑ': '[counterclockwise_arrows]', 'üîÉ': '[clockwise_vertical_arrows]', 'üéµ': '[musical_note]', 'üé∂': '[musical_notes]',
        '‚ûï': '[plus]', '‚ûñ': '[minus]', '‚ûó': '[divide]', '‚úñÔ∏è': '[multiply]', '‚ôæÔ∏è': '[infinity]',
        'üí≤': '[heavy_dollar_sign]', 'üí±': '[currency_exchange]', '‚Ñ¢Ô∏è': '[trademark]', '¬©Ô∏è': '[copyright]', '¬ÆÔ∏è': '[registered]',
        '„Ä∞Ô∏è': '[wavy_dash]', '‚û∞': '[curly_loop]', '‚ûø': '[double_curly_loop]', 'üîö': '[end]', 'üîô': '[back]',
        'üîõ': '[on]', 'üîù': '[top]', 'üîú': '[soon]', '‚úîÔ∏è': '[check_mark]', '‚òëÔ∏è': '[check_box_with_check]',
        'üîò': '[radio_button]', 'üî¥': '[red_circle]', 'üü†': '[orange_circle]', 'üü°': '[yellow_circle]', 'üü¢': '[green_circle]',
        'üîµ': '[blue_circle]', 'üü£': '[purple_circle]', '‚ö´': '[black_circle]', '‚ö™': '[white_circle]', 'üü§': '[brown_circle]',
        'üî∫': '[red_triangle_pointed_up]', 'üîª': '[red_triangle_pointed_down]', 'üî∏': '[small_orange_diamond]', 'üîπ': '[small_blue_diamond]', 'üî∂': '[large_orange_diamond]',
        'üî∑': '[large_blue_diamond]', 'üî≥': '[white_square_button]', 'üî≤': '[black_square_button]', '‚ñ™Ô∏è': '[black_small_square]', '‚ñ´Ô∏è': '[white_small_square]',
        '‚óæ': '[black_medium_small_square]', '‚óΩ': '[white_medium_small_square]', '‚óºÔ∏è': '[black_medium_square]', '‚óªÔ∏è': '[white_medium_square]', 'üü•': '[red_square]',
        'üüß': '[orange_square]', 'üü®': '[yellow_square]', 'üü©': '[green_square]', 'üü¶': '[blue_square]', 'üü™': '[purple_square]',
        '‚¨õ': '[black_large_square]', '‚¨ú': '[white_large_square]', 'üü´': '[brown_square]', 'üîà': '[speaker_low_volume]', 'üîá': '[muted_speaker]',
        'üîâ': '[speaker_medium_volume]', 'üîä': '[speaker_high_volume]', 'üîî': '[bell]', 'üîï': '[bell_with_slash]', 'üì£': '[megaphone]',
        'üì¢': '[loudspeaker]', 'üëÅ‚Äçüó®': '[eye_in_speech_bubble]', 'üí¨': '[speech_balloon]', 'üí≠': '[thought_balloon]', 'üóØÔ∏è': '[right_anger_bubble]',
        '‚ô†Ô∏è': '[spade_suit]', '‚ô£Ô∏è': '[club_suit]', '‚ô•Ô∏è': '[heart_suit]', '‚ô¶Ô∏è': '[diamond_suit]', 'üÉè': '[joker]',
        'üé¥': '[flower_playing_cards]', 'üÄÑ': '[mahjong_red_dragon]', 'üçª': '[beer]', 'üßø': '[nazar_amulet]'
    }
    
    # Replace emojis with text descriptions for better text processing
    for emoji, description in emoji_map.items():
        text = text.replace(emoji, description)
    
    return text

def extract_text_content(msg):
    """Extract clean text content from message object"""
    text = msg.get('text', '')
    if isinstance(text, list):
        text_parts = []
        for item in text:
            if isinstance(item, str):
                text_parts.append(item)
            elif isinstance(item, dict) and 'text' in item:
                text_parts.append(item['text'])
        text = ''.join(text_parts)
    
    # Convert emojis to text descriptions for better processing
    text = convert_emojis_to_text(str(text).strip() if text else '')
    
    # Additional text cleaning for better processing
    if text:
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove non-printable characters except common punctuation
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)\[\]\"\'–∞-—è—ë–ê-–Ø–Å]+', '', text)
        text = text.strip()
    
    return text

def setup_fonts():
    """Setup fonts for Cyrillic text support"""
    try:
        # Try to register fonts for Cyrillic support (Windows/macOS/Linux)
        font_paths = [
            'C:/Windows/Fonts/arial.ttf',
            'C:/Windows/Fonts/calibri.ttf',
            'C:/Windows/Fonts/tahoma.ttf',
            '/System/Library/Fonts/Arial.ttf',  # macOS
            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',  # Linux
        ]
        
        font_registered = False
        for font_path in font_paths:
            if os.path.exists(font_path):
                try:
                    pdfmetrics.registerFont(TTFont('CyrillicFont', font_path))
                    font_registered = True
                    print(f"Using font: {font_path}")
                    break
                except Exception as e:
                    continue
        
        if not font_registered:
            print("Warning: No suitable font found for Cyrillic, using default (may show squares)")
            return 'Helvetica'
        
        return 'CyrillicFont'
    except Exception as e:
        print(f"Font setup error: {e}")
        return 'Helvetica'

def extract_person_info(chat_name, messages):
    """Extract person information from chat name and messages"""
    # Clean chat name
    person_name = chat_name.strip()
    
    # Remove common prefixes and suffixes
    person_name = re.sub(r'^@', '', person_name)  # Remove @ prefix
    person_name = re.sub(r'\s+\(.*\)$', '', person_name)  # Remove (info) suffix
    
    # Try to extract telegram username if present
    telegram_username = ""
    if chat_name.startswith('@'):
        telegram_username = chat_name
    
    # Extract first/last name if possible
    name_parts = person_name.split()
    first_name = name_parts[0] if name_parts else person_name
    last_name = " ".join(name_parts[1:]) if len(name_parts) > 1 else ""
    
    return {
        'person_name': person_name,
        'first_name': first_name,
        'last_name': last_name,
        'telegram_username': telegram_username
    }

def create_optimized_pdf_parts(chat_name, messages, output_dir, max_size_kb=200):
    """Create multiple PDF files if chat is too large, optimized for n8n processing"""
    person_info = extract_person_info(chat_name, messages)
    person_name = person_info['person_name']
    total_messages = len(messages)
    
    # Setup font for Cyrillic text
    font_name = setup_fonts()
    
    # Optimized chunk sizing based on content analysis - ADJUSTED for 200KB target
    avg_msg_length = sum(len(msg.get('text', '')) for msg in messages) / max(len(messages), 1)
    
    # Further reduced chunk sizes to reach 200KB target
    if avg_msg_length < 50:
        chunk_size = 25   # Reduced from 30
    elif avg_msg_length < 150:
        chunk_size = 18   # Reduced from 20  
    else:
        chunk_size = 12   # Reduced from 15
    
    # More conservative estimation for 200KB files
    estimated_kb_per_chunk = max(1.2, avg_msg_length * 0.005)  # Further reduced multiplier
    max_chunks_per_file = int((max_size_kb * 0.8) / estimated_kb_per_chunk)  # 80% target for more safety
    max_chunks_per_file = max(12, min(max_chunks_per_file, 100))  # Lower bounds
    
    # Group messages into chunks with better memory efficiency
    all_chunks = []
    
    for i in range(0, len(messages), chunk_size):
        chunk = messages[i:i+chunk_size]
        chunk_texts = []
        
        for idx, msg in enumerate(chunk):
            text = msg.get('text', '').strip()
            direction = msg.get('direction', '?')
            # Calculate global message number in chat
            global_msg_num = i + idx + 1
            
            if text:
                # Optimized text cleaning - shorter text length for 200KB files
                text = re.sub(r'\s+', ' ', text)
                text = text[:500]  # Reduced from 600 to 500
                
                # Add chronological numbering to message format
                if direction == '>':
                    chunk_texts.append(f"[{global_msg_num}/{total_messages}] Me: {text}")
                else:
                    chunk_texts.append(f"[{global_msg_num}/{total_messages}] From {person_name}: {text}")
        
        if chunk_texts:
            # Join with separator optimized for vector search
            combined_text = " | ".join(chunk_texts)
            all_chunks.append(combined_text)
    
    # Memory-efficient file creation
    total_chunks = len(all_chunks)
    if total_chunks <= max_chunks_per_file:
        # Single file
        success, chunks = create_single_pdf_file(chat_name, all_chunks, output_dir, person_info, total_messages, font_name)
        return [(f"{sanitize_filename(chat_name)}.pdf", success, chunks)]
    else:
        # Multiple files with optimized splitting
        files_created = []
        chunks_per_file = max_chunks_per_file
        file_count = (total_chunks + chunks_per_file - 1) // chunks_per_file
        
        for file_idx in range(file_count):
            start_idx = file_idx * chunks_per_file
            end_idx = min(start_idx + chunks_per_file, total_chunks)
            file_chunks = all_chunks[start_idx:end_idx]
            
            # Optimized filename generation
            base_name = sanitize_filename(chat_name)
            part_filename = f"{base_name}_part{file_idx + 1}of{file_count}.pdf"
            
            success, chunks = create_single_pdf_file(
                f"{chat_name} (Part {file_idx + 1}/{file_count})", 
                file_chunks, 
                output_dir, 
                person_info, 
                total_messages, 
                font_name,
                custom_filename=part_filename
            )
            
            files_created.append((part_filename, success, chunks))
        
        return files_created

def create_single_pdf_file(chat_name, chunks, output_dir, person_info, total_messages, font_name, custom_filename=None):
    """Create a single PDF file from chunks with person name in metadata"""
    if custom_filename:
        filename = custom_filename
        filepath = os.path.join(output_dir, filename)
    else:
        filename = f"{sanitize_filename(chat_name)}.pdf"
        filepath = os.path.join(output_dir, filename)
    
    person_name = person_info['person_name']
    
    # Create PDF document with person name in title metadata only
    doc = SimpleDocTemplate(filepath, pagesize=A4, 
                           rightMargin=40, leftMargin=40,
                           topMargin=40, bottomMargin=40,
                           title=person_name,  # Person name in PDF metadata for identification
                           author="",  # Empty author
                           subject="",  # Empty subject
                           creator="",  # Empty creator
                           keywords="")  # Empty keywords
    
    # Container for PDF content
    story = []
    
    # Create styles
    styles = getSampleStyleSheet()
    
    # Simple text style optimized for readability
    text_style = ParagraphStyle(
        'CleanText',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=10,
        spaceAfter=4,
        leftIndent=0,
        rightIndent=0,
        leading=12
    )
    
    # Add chunks to PDF - clean text only, no headers
    for chunk_text in chunks:
        text_para = Paragraph(chunk_text, text_style)
        story.append(text_para)
        story.append(Spacer(1, 6))
    
    # Build PDF
    try:
        doc.build(story)
        return True, len(chunks)
    except Exception as e:
        print(f"Error creating PDF for {chat_name}: {e}")
        return False, 0

def process_telegram_chats_optimized(input_file='result.json'):
    """Main function to process Telegram chats and create optimized PDFs for n8n"""
    print("Creating optimized PDFs for n8n processing (max 200KB per file)...")
    
    # Load chat data with better error handling
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Error: {input_file} not found!")
        return False
    except json.JSONDecodeError as e:
        print(f"‚ùå Error: Invalid JSON in {input_file}: {e}")
        return False
    except Exception as e:
        print(f"‚ùå Error loading {input_file}: {e}")
        return False
    
    chat_list = data.get('chats', {}).get('list', [])
    if not chat_list:
        print("‚ùå No chats found in the data!")
        return False
    
    print(f"üìÇ Found {len(chat_list)} chats to process")
    
    # Create output directory
    try:
        os.makedirs('chats_clean_pdf', exist_ok=True)
    except Exception as e:
        print(f"‚ùå Error creating output directory: {e}")
        return False
    
    # Processing counters
    processed_count = 0
    skipped_count = 0
    total_messages = 0
    total_chunks = 0
    total_files = 0
    
    # Summary data for n8n metadata
    summary_data = []
    
    # Process each chat with progress tracking
    for idx, chat in enumerate(chat_list, 1):
        if chat.get('type') != 'personal_chat':
            skipped_count += 1
            continue
        
        chat_name = chat.get('name') or f"Chat_{chat.get('id', 'Unknown')}"
        messages = chat.get('messages', [])
        
        if not messages:
            print(f"‚ö†Ô∏è  Skipping {chat_name}: No messages found")
            skipped_count += 1
            continue
        
        print(f"üîÑ Processing [{idx}/{len(chat_list)}]: {chat_name} ({len(messages)} messages)")
        
        chat_messages = []
        
        # Extract and process messages with optimization
        for msg in messages:
            if msg.get('type') != 'message':
                continue
            
            text_content = extract_text_content(msg)
            if not text_content or len(text_content.strip()) < 2:  # Skip very short messages
                continue
            
            # Determine message direction (sent by me or received)
            sender = msg.get('from', 'Unknown')
            sender_id = msg.get('from_id', '')
            
            # Check if message was sent by me (Danil) or received from chat partner
            is_from_me = (sender and 'Danil' in sender) or sender_id == 'user904048578'
            direction = '>' if is_from_me else '<'
            
            # Store essential data with direction
            chat_msg = {
                'text': text_content,
                'direction': direction
            }
            chat_messages.append(chat_msg)
        
        if not chat_messages:
            print(f"‚ö†Ô∏è  Skipping {chat_name}: No valid messages after processing")
            skipped_count += 1
            continue
        
        # Create clean PDF files (potentially multiple parts)
        try:
            files_created = create_optimized_pdf_parts(chat_name, chat_messages, 'chats_clean_pdf')
        except Exception as e:
            print(f"‚ùå Error processing {chat_name}: {e}")
            skipped_count += 1
            continue
        
        if files_created:
            processed_count += 1
            total_messages += len(chat_messages)
            
            # Extract person info for summary
            person_info = extract_person_info(chat_name, chat_messages)
            
            # Count sent vs received messages
            sent_count = sum(1 for msg in chat_messages if msg['direction'] == '>')
            received_count = sum(1 for msg in chat_messages if msg['direction'] == '<')
            
            # Process each created file
            for filename, success, chunk_count in files_created:
                if success:
                    total_files += 1
                    total_chunks += chunk_count
                    
                    # Get file size safely
                    try:
                        pdf_path = os.path.join('chats_clean_pdf', filename)
                        file_size = os.path.getsize(pdf_path) / 1024
                    except:
                        file_size = 0
                    
                    # Determine if this is a multi-part file
                    is_multipart = 'part' in filename.lower()
                    part_info = ""
                    if is_multipart:
                        part_match = re.search(r'part(\d+)of(\d+)', filename.lower())
                        if part_match:
                            part_info = f" [Part {part_match.group(1)}/{part_match.group(2)}]"
                    
                    print(f"   ‚úÖ {filename}: {chunk_count} chunks ({file_size:.1f} KB){part_info}")
                    
                    # Add to summary for n8n
                    summary_data.append({
                        'filename': filename,
                        'person_name': person_info['person_name'],
                        'first_name': person_info['first_name'],
                        'last_name': person_info['last_name'],
                        'telegram_username': person_info['telegram_username'],
                        'original_chat': chat_name,
                        'is_multipart': is_multipart,
                        'chunk_count': chunk_count,
                        'file_size_kb': round(file_size, 1),
                        'total_messages_in_chat': len(chat_messages),
                        'sent_count': sent_count,
                        'received_count': received_count
                    })
                else:
                    print(f"   ‚ùå {filename}: Failed to create PDF")
            
            # Summary for this chat
            if len(files_created) > 1:
                total_size = sum(os.path.getsize(os.path.join('chats_clean_pdf', f[0])) / 1024 
                               for f in files_created if f[1] and os.path.exists(os.path.join('chats_clean_pdf', f[0])))
                print(f"   üìä Total: {len(chat_messages)} messages (Me:{sent_count}, From {person_info['person_name']}:{received_count}) ‚Üí {len(files_created)} files ({total_size:.1f} KB)")
            else:
                print(f"   üìä Total: {len(chat_messages)} messages (Me:{sent_count}, From {person_info['person_name']}:{received_count})")
        else:
            print(f"‚ùå {chat_name}: Failed to create any PDF files")
            skipped_count += 1
    
    # Save summary for n8n workflow
    try:
        with open('chats_clean_pdf/metadata_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        print(f"üìã Metadata saved: metadata_summary.json")
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not save metadata: {e}")
    
    # Final summary
    print(f"\nüéØ Processing completed successfully!")
    print(f"üìÅ Output location: chats_clean_pdf/ directory")
    print(f"üìä Results:")
    print(f"   ‚úÖ Processed: {processed_count} chats")
    print(f"   ‚ö†Ô∏è  Skipped: {skipped_count} chats")
    print(f"   üìÑ Created: {total_files} PDF files")
    print(f"   üí¨ Total messages: {total_messages}")
    print(f"   üì¶ Total chunks: {total_chunks}")
    
    if processed_count > 0:
        avg_messages_per_chat = total_messages / processed_count
        avg_chunks_per_file = total_chunks / total_files if total_files > 0 else 0
        print(f"   üìà Average: {avg_messages_per_chat:.1f} messages/chat, {avg_chunks_per_file:.1f} chunks/file")
    
    print(f"\nüí° Optimizations applied:")
    print(f"   ‚úÖ Dynamic chunk sizing based on message length")
    print(f"   ‚úÖ Memory-efficient processing")
    print(f"   ‚úÖ Optimized file size management (max 200KB)")
    print(f"   ‚úÖ Clean message format for n8n processing")
    print(f"   ‚úÖ Enhanced error handling and progress tracking")
    print(f"   ‚úÖ Emoji conversion and text normalization")
    
    print(f"\nüîß n8n Integration Tips:")
    print(f"   1. Text Splitter settings: chunk_size=800, overlap=200")
    print(f"   2. Process files in batches of 5-8 for optimal memory usage")
    print(f"   3. Search patterns: 'Me:', 'From [NAME]:', person names")
    print(f"   4. Use metadata_summary.json for person identification")
    print(f"   5. Vector dimensions: 1536 (OpenAI) or 768 (local models)")
    print(f"   6. Recommended embedding model: text-embedding-ada-002")
    print(f"   7. PDF text extraction: use 'pdf-parse' node before text splitter")
    
    # Show multipart files info
    large_chats = [item for item in summary_data if item['is_multipart']]
    if large_chats:
        print(f"\n‚ö†Ô∏è  Large chats (split into multiple files):")
        multipart_chats = {}
        for item in large_chats:
            chat = item['original_chat']
            if chat not in multipart_chats:
                multipart_chats[chat] = 0
            multipart_chats[chat] += 1
        
        for chat, file_count in multipart_chats.items():
            print(f"   üìÇ {chat}: {file_count} parts")
    else:
        print(f"\n‚úÖ All chats fit in single files (optimal for processing)")
    
    return True

if __name__ == "__main__":
    process_telegram_chats_optimized() 